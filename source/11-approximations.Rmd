# Approximation {#approximation}

> About **TO-DO**

We have talked a lot about preparing specific quantum states and constructing specific unitary operations, but the space of states of any quantum system is a continuous space, and the set of unitary transformations is also continuous.
It is entirely unrealistic to imagine that in the actual world we will be able to prepare, for example, a qubit *precisely* in the state $\ket{0}$, or to perform a unitary transformation that is *exactly* equal to the controlled-not gate.
*We never have infinite precision in our manipulations of the physical world.*
The good news is that, for all practical purposes, infinite precision is not actually necessary, and we can achieve most of our goals by preparing quantum states and performing quantum operations that are "close enough" to the desired ones.
But what is "close enough", and how do we quantify it?


## Metrics {#metrics}

To begin with, let us work with pure states, and save the problem of dealing with mixed states for a later section.
We will start with the second question: how do we quantify this notion of "close enough"?
The central concept is one with which you are probably already somewhat familiar (we mentioned it in Sections \@ref(bras-and-kets) and \@ref(geometry)), namely that of a **metric**, or **distance**.

::: {.idea latex=""}
Given a set $X$, a **metric** (or **distance**) on $X$ is a function $d\colon X\times X\to\mathbb{R}_{\geq0}$ such that

- **Identity of indiscernibles:** $d(a,b)=0$ if and only if $a=b$
- **Symmetry:** $d(a,b)=d(b,a)$ for all $a,b\in X$
- **Triangle inequality:** $d(a,c)\leq d(a,b)+d(b,c)$ for all $a,b,c\in X$.
:::

::: {.technical title="Generalisations of metrics" latex=""}
**TO-DO** talk about pseudometrics, quasimetrics (nice real life examples), and semimetrics (and maybe extended metrics)
:::

The most common norm is the **Euclidean distance**, that is, distance between two points in Euclidean space.
Given points $A=(a_1,a_2,\ldots,a_n)$ and $B=(b_1,b_2,\ldots,b_n)$ in $\mathbb{R}^n$, their Euclidean distance is
$$
 \sqrt{|b_1-a_1|^2 + |b_2-a_2|^2+\ldots +|b_n-a_n|^2}.
$$
But we already know that Euclidean space $\mathbb{R}^n$ is more than just a set: it is a vector space.
This means that we don't just have a metric space (i.e. a set with a metric), but instead a **normed vector space**, where the **norm** $\|\cdot\|$ of a vector is defined to be the distance of that vector from the origin: $\|a\|\coloneqq d(a,0)$.

It turns out that this norm (and thus this metric) actually arises from a more fundamental structure, namely that of the **inner product**.
Returning to the bra-ket notation, we recall that the norm of any vector $\ket{a}$ is exactly $\|a\|=\sqrt{\braket{a}{a}}$, and thus the distance between any two vectors $\ket{a},\ket{b}$ is exactly $d(\ket{a},\ket{b})=\|\ket{b}-\ket{a}\|$ (though for simplicity we sometimes write this as $\|b-a\|$ instead, or even $\|a-b\|$, since this is equal).
This norm is also called the **$2$-norm**, or the **$\ell^2$-norm** (for reasons that we will come back to in Section \@ref(more-operator-norms)), and is defined for any finite-dimensional Hilbert space $\mathbb{C}^n$ using the fact that $\mathbb{C}\cong\mathbb{R}^2$, so that $\|x+iy\|\coloneqq\|(x,y)\|=\sqrt{x^2+y^2}$.

Before moving on to talk about state vectors, let us first discuss one other metric space which shows up in information theory (both classical and quantum).
The space^[You can think of this as just a set, but we have already seen that this is actually a vector space over $\mathbb{Z}/2\mathbb{Z}$, where addition corresponds to $\texttt{XOR}$.] of binary strings (of some fixed length $n$) admits a metric known as the **Hamming distance**.
This is defined quite simply as "the number of positions at which the corresponding bits are different".
For example,
$$
  d(0101101011,1101110111) = 4
$$
since these two strings differ in four bits:
$$
  \begin{array}{cccccccccc}
    0&1&0&1&1&0&1&0&1&1
  \\1&1&0&1&1&1&0&1&1&1
  \\\hline
    !&\checkmark&\checkmark&\checkmark&\checkmark&!&!&!&\checkmark&\checkmark
  \end{array}
$$

More formally, if we define the **Hamming weight** of a binary string of length $n$ as the number of bits equal to $1$, then the Hamming distance between two strings is simply the Hamming weight of their difference (where subtraction is calculated in $\mathbb{Z}/2\mathbb{Z}$, i.e. $\mod2$).
We leave the proof that this is indeed a metric as an exercise (Exercise \@ref(hamming-distance)).


## How far apart are two quantum states?

Given two pure states, $\ket{u}$ and $\ket{v}$, we could try to measure the distance between them using the Euclidean distance $\|u-v\|$.
This works for vectors, but has some drawbacks when it comes to quantum states.
Recall that a quantum state is not represented by just a unit vector, but by a **ray**, i.e. a unit vector times an arbitrary phase factor.
Multiplying a state vector by an overall phase factor has no physical effect: the two unit vectors $\ket{u}$ and $e^{i\phi}\ket{u}$ describe the same state.
So, in particular, we want the distance between $\ket{u}$ and $-\ket{u}$ to be zero, since these describe the same quantum state.
But if we were to use the Euclidean distance, then we would have that $\|u-(-u)\|=\|u+u\|=2$, which is actually as far apart as the two unit vectors can be!

One solution to this problem is to define the distance between $\ket{u}$ and $\ket{v}$ as the *minimum* over all phase factors, i.e.
$$
  d(u,v)\coloneqq \min_{\phi\in[0,2\pi)}\Big\{\|u-e^{i\phi}v\|\Big\}.
$$
But with some algebraic manipulation we can actually figure out what this minimum is without calculating any of the other values.

We first express the square of the distance between any two vectors in terms of their inner product:
$$
  \begin{aligned}
    \|u-v\|^2
    &= \braket{u-v}{u-v}
  \\&= \braket{u}{u} - \braket{u}{v} - \braket{v}{u} + \braket{v}{v}
  \\&= \|u\|^2 +\|v\|^2 - 2\Re\braket{u}{v}
  \end{aligned}
$$
(where $\Re(z)$ is the real part of the complex number $z$).
Then we can write the Euclidean distance between state vectors as
$$
  \|u-v\| = \sqrt{2(1-\Re\braket{u}{v})}.
$$
Now if we want to minimise this expression over all rotations^[Recall that multiplication by a complex number corresponds to rotation and scaling, and so multiplication by a phase factor (which is always of unit length) corresponds to just rotation.] of $v$, then we want $\braket{u}{v}$ to be real and as large as possible, i.e. for $\braket{u}{v}=|\braket{u}{v}|$.
This gives us a definition of distance.

::: {.idea latex=""}
The **state distance** between two state vectors $\ket{u}$ and $\ket{v}$ is
$$
  d(u,v) \coloneqq \sqrt{2(1-|\braket{u}{v}|)}.
$$
:::

Note that we sometimes write the state distance as $\|u-v\|$, and we might refer to it as "Euclidean distance", which is an abuse of notation: really we should be writing $\min\{\|\ket{u}-e^{i\varphi}\ket{v}\|\}$.
But this sort of thing happens a lot in mathematics^[In computer science lingo, this is what you might call **operator overloading**.], and it's good to get used to it.
The justification is that, as we have already said, the usual Euclidean distance doesn't really make great sense for state vectors (because of this vector vs. ray distinction), and so if we *know* that $\ket{u}$ and $\ket{v}$ are state vectors then writing $\|u-v\|$ (which is already shorthand for $\|\ket{u}-\ket{v}\|$) should suggest "oh, they mean the version of $\|\cdot\|$ that *makes sense for state vectors*, where we take a minimum".

::: {.idea latex=""}
For *small* values of $d(u,v)=\|u-v\|$, we can think of this distance as being the angle between the two unit vectors.
Indeed, if we think of Euclidean (unit) vectors, then the difference $v-u$ is, for sufficiently small $\|u-v\|$, just the angle between the two unit vectors (expressed in radians), because a small segment of a circle "almost" looks like a triangle.

```{r,engine='tikz',fig.width=1.5}
\usetikzlibrary{arrows.meta}
\definecolor{primary}{RGB}{177,98,78}
\definecolor{secondary}{RGB}{91,132,177}
\newcommand{\ket}[1]{|#1\rangle}
\begin{tikzpicture}
  \draw [thick,-Latex,primary] (0,0) to node[below right]{$u$} (2,2);
  \draw [thick,-Latex,rotate=25,secondary](0,0) to node[left]{$v$} (2,2);
  \draw [thick,-Latex] (2,2) to node[above right]{$v-u$} (.97,2.65);
\end{tikzpicture}
```

Alternatively (and more formally), we can see this by writing $|\braket{u}{v}|=\cos\alpha\approx1-\alpha^2/2$, since then
$$
  \|u-v\|
  = \sqrt{2(1-|\braket{u}{v}|)}
  \approx \alpha.
$$

This can certainly help with intuition, but extra care must always be taken when dealing with complex vector spaces, since our geometric intuition breaks down rapidly in (complex) dimension higher than $1$.
:::

As you might hope, two state vectors which are close to one another give similar statistical predictions.
In order to see this, pick a measurement (any measurement) and consider one partial outcome described by a projector $\proj{a}$.
What can we say about the difference between the two probabilities
$$
  \begin{aligned}
    p_u &= |\braket{a}{u}|^2
  \\p_v &= |\braket{a}{v}|^2
  \end{aligned}
$$
if we know that $\|u-v\|\leq\varepsilon$?

Well, first of all, let us introduce two classic tricks that are almost always useful when dealing with inequalities --- the first holds in any normed vector space, and the latter in any inner product space.

- the **reverse triangle inequality**:
    $$
      \Big|\|u\|-\|v\|\Big| \leq \|u-v\|
    $$
- the [**Cauchy--Schwartz inequality**](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality):^[This is arguably *the most useful* mathematical inequality that we have!]
    $$
      \braket{u}{v}^2 \leq \braket{u}{u}\braket{v}{v}
    $$
    or, equivalently (by taking square roots),
    $$
      |\braket{u}{v}| \leq \|u\|\|v\|.
    $$
    Furthermore, the two sides of the inequality are equal *if and only if* $\ket{u}$ and $\ket{v}$ are linearly dependent.

Using these, we see that
$$
  \begin{aligned}
    |p_u-p_v|
    &= \Big| |\braket{a}{u}|^2 - |\braket{a}{v}|^2 \Big|
  \\&= \Big| \Big( |\braket{a}{u}| + |\braket{a}{v}| \Big) \Big( |\braket{a}{u}| - |\braket{a}{v}| \Big) \Big|
  \\&\leq 2\Big| |\braket{a}{u}| - |\braket{a}{v}| \Big|
  \\&\leq 2\Big| \braket{a}{u} - \braket{a}{v} \Big|
  \\&\leq 2\|a\|\|u-v\|
  \\&= 2\|u-v\|.
  \end{aligned}
$$
So if $\|u-v\|\leq\varepsilon$, then $|p_u-p_v|\leq2\varepsilon$.

Again, we can appeal to some geometric intuition if we pretend that $\ket{u}$ and $\ket{v}$ are Euclidean vectors instead of rays.
Write
$$
  \begin{aligned}
    |\braket{a}{u}| &= \cos(\alpha)
  \\|\braket{a}{v}| &= \cos(\alpha+\varepsilon)
  \end{aligned}
$$
where $\varepsilon$ is the (very small) angle between $\ket{u}$ and $\ket{v}$, whence $\|u-v\|=\varepsilon$.
Then
$$
  \begin{aligned}
    |\braket{a}{u}|^2 - |\braket{a}{v}|^2
    &= \cos^2(\alpha) - \cos^2(\alpha+\varepsilon)
  \\&\approx \varepsilon\sin(2\alpha)
  \\&\leq \varepsilon.
  \end{aligned}
$$
As an interesting exercise, you might try to explain why this approach gives a tighter bound ($\varepsilon$ instead of $2\varepsilon$).


## Fidelity

Sometimes, when quantifying closeness of states, the *inner product* is a more convenient tool than the distance/norm.
Analogous to how we define the distance between states $\ket{u}$ and $\ket{v}$ as $d(u,v)=\|u-v\|$, we define the **fidelity** between them as
$$
  F(u,v)\coloneqq |\braket{u}{v}|^2.
$$
This is *not* a metric, but it does have some similarly nice properties: for example, $F(u,v)=1$ when the two states are identical, and $F(u,v)=0$ when the two states are orthogonal (which means that they are "as different as possible").
Intuitively, we can understand fidelity as the probability that the state $\ket{u}$ (resp. $\ket{v}$) would pass a test for being in state $\ket{v}$ (resp. $\ket{u}$).
In other words, if we perform an orthogonal measurement on $\ket{u}$ that has two outcomes ($\texttt{true}$ if the state is $\ket{v}$; $\texttt{false}$ if the state is orthogonal to $\ket{v}$), then the fidelity $F(u,v)=|\braket{u}{v}|^2$ is exactly the probability that we measure the outcome $\texttt{true}$.

Recall our definition of state distance:
$$
  d(u,v) = \sqrt{2(1-|\braket{u}{v}|)}
$$
This gives us a relation between distance and fidelity: once we know one, we can easily calculate the other.
However, everything we have said so far applies only to *pure* states --- we will see how the mixed state case is slightly more complicated shortly.

One final remark: as another example of the many inconsistencies in the literature, some authors define $F(u,v)$ to be $|\braket{u}{v}|$ instead of $|\braket{u}{v}|^2$.
Whenever we say fidelity, we mean the latter: $|\braket{u}{v}|^2$.


## Approximating unitaries

So now we know a bit about how norms (or metrics, or inner products) can help us to understand distance between state vectors, can we say something similar about quantum evolutions?
Say we have unitary operators $U$ and $V$ acting on the same Hilbert space, where $U$ is some "target" unitary that we *want* to implement in a real-life circuit, and $V$ is an "approximate" unitary that we *can actually* implement in practice.
We say that **$V$ approximates $U$ with precision $\varepsilon$**, or that $U$ and $V$ are **$\varepsilon$-close**, if^[Note that $V$ approximates $U$ with precision $\varepsilon$ if and only if $U$ approximates $V$ with precision $\varepsilon$. Even though we might think of one as being our ideal unitary and the other as being the best feasible real-life implementation that we can achieve, this is only us giving names to things --- the definition does not care which way round we think of them.]
$$
  \|U-V\| \leq \varepsilon
$$
where $\|\cdot\|$ is some norm on unitary matrices (of the same size), which we would want to satisfy the following property: if $\|U-V\|$ is "small", then $U$ should be hard to distinguish from $V$ when acting on *any* quantum state.
One natural choice of norm which will be suitable for our purposes is the following.

::: {.idea latex=""}
The **operator norm** $\|A\|$ of an operator $A\in\mathcal{B}(\mathcal{H})$ is the maximum length of the vector $A\ket{v}$ over all possible normalised vectors $\ket{v}\in\mathcal{H}$, i.e.
$$
  \|A\| \coloneqq \max_{\ket{v}\in S_{\mathcal{H}}^1\Big\{ |A\ket{v}| \Big\}
$$
(where $S_{\mathcal{H}}^1$ is the unit sphere in $\mathcal{H}$, i.e. the set of vectors of norm $1$).
One can show that $\|A\|$ is equal to the largest singular value^[We discussed singular values abstractly in Section \@ref(the-schmidt-decomposition), but we can be more explicit here: the **singular values** of an operator $A$ are the square roots of the (necessarily non-negative) eigenvalues of the Hermitian operator $A^\dagger A$.] of $A$.

If $A$ is *normal*, then its singular values are exactly the absolute values of its eigenvalues, and so
$$
  \|A\| = \max_{\lambda\in\sigma(A)}\Big\{ |\lambda| \Big\}
$$
(where $\sigma(A)=\{\lambda\in\mathbb{C}\mid\det(A-\lambda\id)=0\}$ is the set of eigenvalues of $A$).
:::

The operator norm satisfies some very useful properties:^[Proving these properties, along with some others, is a good thing to practise --- see Exercise \@ref(operator-norm).]

- If $A$ is normal, then $\|A^\dagger\|=\|A\|$
- $\|A\otimes B\|=\|A\|\|B\|$
- If $U$ is unitary, then $\|U\|=1$
- If $P\neq0$ is an orthogonal projector, then $\|P\|=1$
- **Sub-multiplicativity:** $\|AB\|\leq\|A\|\|B\|$.

Now suppose that some quantum system, initially in state $\ket{\psi}$, evolves according to $U$ or $V$.
Let $P$ be a projector associated with some specific outcome of some measurement that can be performed on the system after either evolution (such as $P=\proj{a}$, as in our earlier example).
Let $p_U$ (resp. $p_V$) be the probability of obtaining the corresponding measurement outcome if the operation $U$ (resp. $V$) was performed.
By definition, we see that
$$
  \begin{aligned}
    |p_U-p_V|
    &= \Big| \braket{\psi|U^\dagger PU}{\psi} - \braket{\psi|V^\dagger PV}{\psi} \Big|
  \\&= \Big| \braket{\psi|U^\dagger P(U-V)}{\psi}+\braket{\psi|(U^\dagger-V^\dagger)PV}{\psi} \Big|
  \\&\leq \Big| \braket{\psi|U^\dagger P(U-V)}{\psi} \Big| + \Big| \braket{\psi|(U^\dagger-V^\dagger)PV}{\psi} \Big|
  \end{aligned}
$$
where the inequality is exactly the triangle inequality.

By an application of the Cauchy--Schwartz inequality^[See Exercise \@ref(operator-norm).] followed by sub-multiplicativity, we then have
$$
  \begin{aligned}
    |p_U-p_V|
    &\leq \|U^\dagger P\|\|U-V\| + \|U^\dagger-V^\dagger\|\|VP\|
  \\&\leq 2\|U-V\|.
  \end{aligned}
$$

This tells us what $\varepsilon$-closeness means: suppose that $V$ and $U$ are $\varepsilon$-close; then if, instead of applying one, we apply the other, and subsequently measure the resulting physical system, we know that the probabilities of any particular outcome in any measurement will differ by *at most* $2\varepsilon$.

Now what about working with *sequences* of unitaries, as we do when we construct quantum circuits?
It turns out that closeness is additive under multiplication of unitaries: if $\|U_1-V_1\|\leq\varepsilon_1$ and $\|U_2-V_2\|\leq\varepsilon_2$, then
$$
  \begin{aligned}
    \|U_2U_1 - V_2V_1\|
    &= \|U_2U_1 - V_2U_1 + V_2U_1 - V_2V_1\|
  \\&= \|(U_2-V_2)U_1 + V_2(U_1-V_1)\|
  \\&\leq \|U_2-V_2\|\|U_1\| + \|V_2\|\|U_1-V_1\|
  \\&= \|U_2-V_2\| + \|U_1-V_1\|
  \\&\leq \varepsilon_1+\varepsilon_2.
  \end{aligned}
$$
We can then apply this argument inductively.

::: {.idea latex=""}
Errors in the approximation of one sequence of unitaries by another accumulate at most linearly in the number of unitary operations:
$$
  \|U_n\cdots U_1 - V_n\cdots V_1\| \leq \sum_{i=1}^n \varepsilon_n
$$
if $\|U_i-V_i\|\leq\varepsilon_i$ for all $i=1,\ldots,n$.
:::

This linear error accumulation relies heavily on the fact that the norm of a unitary operator is equal to $1$; for non-unitary operators, errors could accumulate exponentially, which would make efficient approximations of circuits practically impossible.
Geometrically, this is because unitaries just *rotate* vectors, without scaling them.

Again, we can appeal to some trigonometry.
First note that
$$
  \|U-V\| = \|UV^\dagger-\id\|
$$
since the operator norm is unitarily invariant.^[See Exercise \@ref(operator-norm).]
Since $UV^\dagger$ is also unitary, its eigenvalues are exactly phase factors $e^{i\varphi}$ for $\varphi\in\mathbb{R}$; the corresponding eigenvalue of $UV^\dagger-\id$ has modulus
$$
  |e^{i\varphi}-1| = \sqrt{2}\sqrt{1-\cos\varphi}.
$$
Putting this all together, we see that asking for $\|U-V\|\leq\varepsilon$ is exactly asking for each eigenvalue of $UV^\dagger-\id$ to satisfy $\sqrt{2}\sqrt{1-\cos\varphi}\leq\varepsilon$, which rearranges to
$$
  \cos\varphi \geq 1-\frac{\varepsilon^2}{2}
$$
which is simply $|\varphi|\leq\varepsilon$ for small enough $\varepsilon$.
So $U$ rotates relative to $V$ by (at worst) an angle of order $\varepsilon$, and if we compose unitaries in a sequence then the accumulated rotation increases linearly with the number of unitaries.


## Approximating generic unitaries is hard, but...

Now that we understand approximations of unitary operators, we can revisit the question of universality that we touched upon in Sections \@ref(finite-set-of-universal-gates) and \@ref(universality-again).
Recall that we call a finite set $G$ of gates **universal** if *any* $n$-qubit unitary operator can be approximated (up to an overall phase) to *arbitrary accuracy* by some unitary constructed using only gates from $G$ (and we then call the gates in $G$ **elementary**).
In other words, $G$ is universal if, for any unitary $U$ acting on $n$-qubits and for any $\varepsilon>0$, there exist $U_1,\ldots,U_d\in G$ such that $\widetilde{U}\coloneqq U_d\cdots U_1$ satisfies
$$
  \|\widetilde{U}-e^{i\varphi}U\|\leq\varepsilon
$$
for some phase $\varphi$.

For example, each of the following sets of gates is universal:

- $\{H,\texttt{c-}S\}$
- $\{H,T,\texttt{c-NOT}\}$
- $\{H,S,\texttt{Toff}\}$

where $S$ and $T$ are the $\pi/4$- and $\pi/8$-phase gates (Section \@ref(phase-gates-galore)), $\texttt{c-}S$ is the *controlled* $S$-gate, and $\texttt{Toff}$ is the Toffoli gate (Exercise \@ref(toffoli-gate)).

But now we can be a bit more precise with the question that the notion of universality is trying to answer: given a universal set of gates, how hard is it to approximate any desired unitary transformation with accuracy $\varepsilon$?
That is, *how many gates do we need*?

The answer is *a lot*.
In fact, it is exponential in the number of qubits --- most unitary transformation require large quantum circuits of elementary gates.
We can show this by a counting argument (along with a healthy dose of geometric intuition).

Consider a universal set of gates $G$ consisting of $g$ gates, where each gates acts on no more than $k$ qubits.
How many circuits (acting on $n$-qubits) can we construct using $t$ gates from this set?
We have $g\binom{n}{k}$ choices^[Counting arguments nearly always use [**binomial coefficient** notation](https://en.wikipedia.org/wiki/Binomial_coefficient): $\binom{a}{b}\coloneqq\frac{a!}{b!(b-a)!}$.] for the first gate, since there are $g$ gates, and $\binom{n}{k}$ ways to place it so that it acts on $k$ out of $n$ qubits.
The same holds for all subsequent gates, and so we can build no more than
$$
  \left( g\binom{n}{k} \right)^t
$$
circuits of size $t$ from $G$.
What is important is that $g\binom{n}{k}$ is *polynomial* in $n$, and $g$ and $k$ are fixed constants, so we will write this upper bound as
$$
  (\mathrm{poly}(n))^t.
$$
In more geometric language, we have shown that, with $t$ gates, we can generate $(\mathrm{poly}(n))^t$ points in the space $U(N)$ of unitary transformations on $n$-qubits, where $N=2^n$.
Now imagine drawing a ball of radius $\varepsilon$ (in the operator norm) centred at each of these points --- we want these balls to cover the entire unitary group $U(N)$, since this then says that any unitary is within distance $\varepsilon$ of a circuit built from $t$ gates in $G$.
We will not get into the details of the geometry of $U(N)$, but simply use the fact that a ball of radius $\varepsilon$ in $U(N)$ has volume proportional to $\varepsilon^{N^2}$, whereas the volume of $UN)$ itself is proportional to $C^{N^2}$ for some fixed constant $C$.
So we want
$$
  \varepsilon^{N^2}(\mathrm{poly}(n))^t \geq C^{N^2}
$$
which (after some algebraic manipulation) requires that
$$
  t \geq 2^{2n}\frac{\log(C/\varepsilon)}{\log(\mathrm{poly}(n))}.
$$
In words, *the scaling is exponential in $n$ but only logarithmic in $1/\varepsilon$*.

::: {.idea latex=""}
When we add qubits, the space of possible unitary operations grows very rapidly, and we have to work exponentially hard if we want to approximate the resulting unitaries with some prescribed precision.
If, however, we fix the number of qubits and instead ask for better and better approximations, then things are much easier, since we only have to work logarithmically hard.
:::

The snag is that this counting argument does not give us any hints as to how we can actually build such approximations.
A more constructive approach is to pick a set of universal gates and play with them, building more and more complex circuits.
There is an important theorem in this direction that tells us that it does not matter much which particular universal set of gates we choose to start with.

::: {.idea latex=""}
**The [Solovay--Kitaev Theorem](https://en.wikipedia.org/wiki/Solovay%E2%80%93Kitaev_theorem).**
Choose any two universal sets of gates that are closed under inverses.^[The notion of "being closed under inverses" is slightly weaker than you might first think: it means that the inverse of any gate in the set can be (exactly) constructed from a *finite* sequence of gates in the set; it does *not* mean that the inverses have to be elementary gates themselves.]
Then any $t$-gate circuit built from one set of gates can be implemented to precision $\varepsilon$ using a $t\mathrm{poly}(\log(t/\varepsilon))$-gate circuit built from the other set.
Furthermore, there is an efficient classical algorithm for finding this circuit.
:::

Since errors accumulate linearly, it suffices to approximate each gate from one set to accuracy $\varepsilon/t$, which can be achieved by using a $\mathrm{poly}(\log(t/\varepsilon))$-gate circuit built from the other set.
So we can efficiently convert (constructively, via some efficient classical algorithm) between universal sets of gates with overhead $\mathrm{poly}(\log(1/\varepsilon)))$, i.e. $\log^c(1/\varepsilon)$ for some constant $c$.
For all practical purposes, we want to minimise $c$, but the counting argument above shows that the best possible exponent is $1$, so the real question is *can we get close to this lower bound*?
In general, we do not know.
However, for some universal sets of gates we have *nearly* optimal constructions.
For example, the set $\{H,T\}$ can be used to approximate arbitrary *single-qubit* unitaries to accuracy $\varepsilon$ using $\log(1/\varepsilon)$ many gates, instead of $\mathrm{poly}(\log(1/\varepsilon))$, and the circuits achieving this improved overhead cost can be efficiently constructed (for example, by the [Matsumoto--Amano construction](https://arxiv.org/abs/0806.3834).)


## How far away are two probability distributions?


Before we switch gears and discuss how to generalise state distance to density operators, let us first take a look at distances between probability distributions.
What does it mean to say that two probability distributions (over the same index set) are similar to one another?

::: {.idea latex=""}
The **trace distance**^[Also known as the **variation** distance, $L_1$ distance, **statistical** distance, or **Kolmogorov** distance.] between probability distributions $p(k)$ and $q(k)$ is
$$
  d(p,q) \coloneqq \frac{1}{2}\sum_k |p(k)-q(k)|.
$$
:::

This is indeed a distance: it satisfies all the necessary properties.^[**Exercise.** Show that this distance satisfies the triangle inequality.]
It also has a rather simple interpretation, as we now explain.
Let $p(k)$ be the *intended* probability distribution of an outcome produced by some ideal device $P$, but suppose that the actual physical device $Q$ is slightly faulty: with probability $1-\varepsilon$ it works exactly as $P$ does, but with probability $\varepsilon$ it goes completely wrong and generates an output according to some arbitrary probability distribution $e(k)$.
What can we say about the probability distribution $q(k)$ of the outcome of such a device?
Well, we can exactly say that
$$
  d(p,q) \leq \varepsilon
$$
by substituting $q(k)=(1-\varepsilon)p(k)+\varepsilon e(k)$.
Conversely, if $d(p,q)=\varepsilon$ then we can represent one of them (say, $q(k)$) as the probability distribution resulting from a process that generates outcomes according to $p(k)$ followed by a process that alters outcome $k$ with total probability not greater than $\varepsilon$.

Note that the normalisation property of probabilities implies that
$$
  \sum_k p(k)-q(k) = 0.
$$
We can split up this sum into two parts: the sum over $k$ for which $p(k)\geq q(k)$, and the sum over $k$ for which $p(k)<q(k)$.
If we call the first part $S$, then the fact that $\sum_k p(k)-q(k)=0$ tells us that the second part must be equal to $-S$.
Thus
$$
  \sum_k |p(k)-q(k)|
  = S+|-S|
  = 2S
$$
whence $S=d(p,q)$.

(ref:probability-distribution-distance-caption) **TO-DO: write caption**

```{r probability-distribution-distance,engine.opts=list(template="tikz2pdf.tex"),fig.cap='(ref:probability-distribution-distance-caption)',fig.width=4}
\definecolor{primary}{RGB}{177,98,78}
\definecolor{secondary}{RGB}{91,132,177}
\begin{tikzpicture}
  \pgfmathdeclarefunction{gauss}{2}{%
    \pgfmathparse{1/(#2*sqrt(2*pi))*exp(-((x-#1)^2)/(2*#2^2))}%
  }
  \begin{axis}[axis y line=none,
    no markers,
    domain=0:10,
    samples=250,
    axis lines*=left,
    xlabel=$k$,
    every axis x label/.style={at=(current axis.right of origin),anchor=west},
    height=5cm,
    width=12cm,
    xtick=\empty,
    xticklabels={$$},
    enlargelimits=false,
    clip=false,
    axis on top,
    grid = major
    ]
  \addplot [fill=gray, draw=none, domain=0:10] {min(gauss(4,1.1),gauss(6.5,0.9))} \closedcycle;
  \addplot [very thick,primary] {gauss(4,1.1)};
  \addplot [very thick,secondary] {gauss(6.5,0.9)};
  \draw [yshift= 1.5cm,xshift=4cm] node {$S$};
  \draw [yshift= 1.5cm,xshift=7cm] node {$S$};
  \draw [yshift= 2.3cm,xshift=2.6cm] node {$p(k)$};
  \draw [yshift= 2.3cm,xshift=8.4cm] node {$q(k)$};
  \end{axis}
\end{tikzpicture}
```

**TO-DO: continue from here**


















## Dealing with density operators

**TO-DO**


## Distinguishing non-orthogonal states, again {#distinguishing-non-orthogonal-states-again}

Let's briefly return to the problem considered in Section @\ref(distinguishing-non-orthogonal-states).
**TO-DO: include end of ยง1 in the draft**


## How accurate is accurate enough?

**TO-DO**


## Operator decompositions

**TO-DO**


## More operator norms

**TO-DO: recall Section \@ref(modifying-the-born-rule); talk about $\ell^p$ norms (skip 10.1 in draft)**


























## *Remarks and exercises* {#remarks-and-exercises-11}

### Hamming distance {#hamming-distance}

Show that the Hamming distance (defined in Section \@ref(metrics)) is indeed a metric.


### Operator norm {#operator-norm}

Prove the following properties of the operator norm:

2. $\|A\otimes B\|=\|A\|\|B\|$ for any operators $A$ and $B$
1. If $A$ is normal, then $\|A^\dagger\|=\|A\|$
3. If $U$ is unitary, then $\|U\|=1$
4. If $P\neq0$ is an orthogonal projector, then $\|P\|=1$.

Using the singular value decomposition^[Recall Section \@ref(the-schmidt-decomposition)], or otherwise, prove that the operator norm has the following two properties for any operators $A$ and $B$:

5. **Unitary invariance:** $\|UAV\|=\|A\|$ for any unitaries $U$ and $V$
6. **Sub-multiplicativity:** $\|AB\|\leq\|A\|\|B\|$.

Recall that we say that $V$ approximates $U$ with precision $\varepsilon$ if $\|U-V\|\leq\varepsilon$.

7. Prove that, if $V$ approximates $U$ with precision $\varepsilon$, then $V^{-1}$ approximates $U^{-1}$ with the same precision $\varepsilon$.

Using the Cauchy--Schwartz inequality, or otherwise, prove the following, for any vector $\ket{\psi}$ and any operators $A$ and $B$:

8. $|\braket{\psi|A^\dagger B}{\psi}|\leq\|A\|\|B\|$.


### Tolerance and precision

Suppose we wish to implement a quantum circuit consisting of gates $U_1,\ldots,U_d$, but we only have available to us gates $V_1,\ldots,V_d$.
Luckily, these gates are pretty good approximations to our desired gates, and the error is uniform: $\|U_i-V_i\|\leq\varepsilon$ for all $i=1,\ldots,d$ for some fixed $\varepsilon$.

We want our approximate circuit to be within some **tolerance** $\delta$ of the desired circuit: the probabilities of different outcomes of $V=V_d\cdots V_1$ should be be within $\delta$ of the "correct" probabilities of the different outcomes of $U=U_d\cdots U_1$, i.e. $|p_U-p_V|\leq\delta$.

How small must $\varepsilon$ be with respect to $\delta$ in order for us to achieve this?

*Hint: recall that $|p_U-p_V|\leq2\|U-V\|$.*


### Distinguishability and the trace distance

**TO-DO: check that the notation here (e.g. for trace distance) is consistent**

Say we have a physical system which is been prepared in one of two states (say, $\rho_1$ and $\rho_2$), each with equal probability.
Then a *single* measurement can distinguish between the two preparations with probability *at most* $\frac{1}{2}[1+d_{\tr}(\rho_1,\rho_2)]$.

1. Suppose that $\rho_1$ and $\rho_2$ commute.^[The commutativity assumption makes this problem essentially a special case of a purely classical one: distinguishing between two probability distributions.]
    Using the spectral decompositions of $\rho_1$ and $\rho_2$ in their common eigenbasis, describe the optimal measurement that can distinguish between the two states. What is its probability of success?

2. Suppose that you are given one randomly selected qubit from a pair in the state
    $$
      \ket{\psi} =
      \frac{1}{\sqrt{2}}\left(
        \ket{0}\otimes\left(
          \sqrt{\frac23}\ket{0}
          - \sqrt{\frac13}\ket{1}
        \right)
        + \ket{1}\otimes\left(
          \sqrt{\frac23}\ket{0}
          + \sqrt{\frac13}\ket{1}
        \right)
      \right)
    $$
    (which we have already seen in Exercise \@ref(some-density-operator-calculations)).
    What is the maximal probability with which you can determine which qubit (either the first or the second) you were given?
