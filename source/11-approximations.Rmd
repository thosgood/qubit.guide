# Approximation {#approximation}

> About **TO-DO**

We have talked a lot about preparing specific quantum states and constructing specific unitary operations, but the space of states of any quantum system is a continuous space, and the set of unitary transformations is also continuous.
It is entirely unrealistic to imagine that in the actual world we will be able to prepare, for example, a qubit *precisely* in the state $\ket{0}$, or to perform a unitary transformation that is *exactly* equal to the controlled-not gate.
*We never have infinite precision in our manipulations of the physical world.*
The good news is that, for all practical purposes, infinite precision is not actually necessary, and we can achieve most of our goals by preparing quantum states and performing quantum operations that are "close enough" to the desired ones.
But what is "close enough", and how do we quantify it?


## Metrics {#metrics}

To begin with, let us work with pure states, and save the problem of dealing with mixed states for a later section.
We will start with the second question: how do we quantify this notion of "close enough"?
The central concept is one with which you are probably already somewhat familiar (we mentioned it in Sections \@ref(bras-and-kets) and \@ref(geometry)), namely that of a **metric**, or **distance**.

::: {.idea latex=""}
Given a set $X$, a **metric** (or **distance**) on $X$ is a function $d\colon X\times X\to\mathbb{R}_{\geq0}$ such that

- **Identity of indiscernibles:** $d(a,b)=0$ if and only if $a=b$
- **Symmetry:** $d(a,b)=d(b,a)$ for all $a,b\in X$
- **Triangle inequality:** $d(a,c)\leq d(a,b)+d(b,c)$ for all $a,b,c\in X$.
:::

::: {.technical title="Generalisations of metrics" latex=""}
**TO-DO** talk about pseudometrics, quasimetrics (nice real life examples), and semimetrics (and maybe extended metrics)
:::

The most common norm is the **Euclidean distance**, that is, distance between two points in Euclidean space.
Given points $A=(a_1,a_2,\ldots,a_n)$ and $B=(b_1,b_2,\ldots,b_n)$ in $\mathbb{R}^n$, their Euclidean distance is
$$
 \sqrt{|b_1-a_1|^2 + |b_2-a_2|^2+\ldots +|b_n-a_n|^2}.
$$
But we already know that Euclidean space $\mathbb{R}^n$ is more than just a set: it is a vector space.
This means that we don't just have a metric space (i.e. a set with a metric), but instead a **normed vector space**, where the **norm** $\|\cdot\|$ of a vector is defined to be the distance of that vector from the origin: $\|a\|\coloneqq d(a,0)$.

It turns out that this norm (and thus this metric) actually arises from a more fundamental structure, namely that of the **inner product**.
Returning to the bra-ket notation, we recall that the norm of any vector $\ket{a}$ is exactly $\|a\|=\sqrt{\braket{a}{a}}$, and thus the distance between any two vectors $\ket{a},\ket{b}$ is exactly $d(\ket{a},\ket{b})=\|\ket{b}-\ket{a}\|$ (though for simplicity we sometimes write this as $\|b-a\|$ instead, or even $\|a-b\|$, since this is equal).
This norm is also called the **$2$-norm**, or the **$\ell^2$-norm**, and is defined for any finite-dimensional Hilbert space $\mathbb{C}^n$ using the fact that $\mathbb{C}\cong\mathbb{R}^2$, so that $\|x+iy\|\coloneqq\|(x,y)\|=\sqrt{x^2+y^2}$.

::: {.technical title="p-norms" latex=""}
**TO-DO:** a *little* bit about $L^p$ and $\ell^p$
:::

Before moving on to talk about state vectors, let us first discuss one other metric space which shows up in information theory (both classical and quantum).
The space^[You can think of this as just a set, but we have already seen that this is actually a vector space over $\mathbb{Z}/2\mathbb{Z}$, where addition corresponds to $\texttt{XOR}$.] of binary strings (of some fixed length $n$) admits a metric known as the **Hamming distance**.
This is defined quite simply as "the number of positions at which the corresponding bits are different".
For example,
$$
  d(0101101011,1101110111) = 4
$$
since these two strings differ in four bits:
$$
  \begin{array}{cccccccccc}
    0&1&0&1&1&0&1&0&1&1
  \\1&1&0&1&1&1&0&1&1&1
  \\\hline
    !&\checkmark&\checkmark&\checkmark&\checkmark&!&!&!&\checkmark&\checkmark
  \end{array}
$$

More formally, if we define the **Hamming weight** of a binary string of length $n$ as the number of bits equal to $1$, then the Hamming distance between two strings is simply the Hamming weight of their difference (where subtraction is calculated in $\mathbb{Z}/2\mathbb{Z}$, i.e. $\mod2$).
We leave the proof that this is indeed a metric as an exercise (Exercise \@ref(hamming-distance)).


## How far apart are two quantum states?

Given two pure states, $\ket{u}$ and $\ket{v}$, we could try to measure the distance between them using the Euclidean distance $\|u-v\|$.
This works for vectors, but has some drawbacks when it comes to quantum states.
Recall that a quantum state is not represented by just a unit vector, but by a **ray**, i.e. a unit vector times an arbitrary phase factor.
Multiplying a state vector by an overall phase factor has no physical effect: the two unit vectors $\ket{u}$ and $e^{i\phi}\ket{u}$ describe the same state.
So, in particular, we want the distance between $\ket{u}$ and $-\ket{u}$ to be zero, since these describe the same quantum state.
But if we were to use the Euclidean distance, then we would have that $\|u-(-u)\|=\|u+u\|=2$, which is actually as far apart as the two unit vectors can be!

One solution to this problem is to define the distance between $\ket{u}$ and $\ket{v}$ as the *minimum* over all phase factors, i.e.
$$
  d(u,v)\coloneqq \min_{\phi\in[0,2\pi)}\Big\{\|u-e^{i\phi}v\|\Big\}.
$$
But with some algebraic manipulation we can actually figure out what this minimum is without calculating any of the other values.

We first express the square of the distance between any two vectors in terms of their inner product:
$$
  \begin{aligned}
    \|u-v\|^2
    &= \braket{u-v}{u-v}
  \\&= \braket{u}{u} - \braket{u}{v} - \braket{v}{u} + \braket{v}{v}
  \\&= \|u\|^2 +\|v\|^2 - 2\Re\braket{u}{v}
  \end{aligned}
$$
(where $\Re(z)$ is the real part of the complex number $z$).
Then we can write the Euclidean distance between state vectors as
$$
  \|u-v\| = \sqrt{2(1-\Re\braket{u}{v})}.
$$
Now if we want to minimise this expression over all rotations^[Recall that multiplication by a complex number corresponds to rotation and scaling, and so multiplication by a phase factor (which is always of unit length) corresponds to just rotation.] of $v$, then we want $\braket{u}{v}$ to be real and as large as possible, i.e. for $\braket{u}{v}=|\braket{u}{v}|$.
This gives us a definition of distance.

::: {.idea latex=""}
The **state distance** between two state vectors $\ket{u}$ and $\ket{v}$ is
$$
  d(u,v) \coloneqq \sqrt{2(1-|\braket{u}{v}|)}.
$$
:::

Note that we sometimes write the state distance as $\|u-v\|$, and we might refer to it as "Euclidean distance", which is an abuse of notation: really we should be writing $\min\{\|\ket{u}-e^{i\varphi}\ket{v}\|\}$.
But this sort of thing happens a lot in mathematics^[In computer science lingo, this is what you might call **operator overloading**.], and it's good to get used to it.
The justification is that, as we have already said, the usual Euclidean distance doesn't really make great sense for state vectors (because of this vector vs. ray distinction), and so if we *know* that $\ket{u}$ and $\ket{v}$ are state vectors then writing $\|u-v\|$ (which is already shorthand for $\|\ket{u}-\ket{v}\|$) should suggest "oh, they mean the version of $\|\cdot\|$ that *makes sense for state vectors*, where we take a minimum".

::: {.idea latex=""}
For *small* values of $d(u,v)=\|u-v\|$, we can think of this distance as being the angle between the two unit vectors.
Indeed, if we think of Euclidean (unit) vectors, then the difference $v-u$ is, for sufficiently small $\|u-v\|$, just the angle between the two unit vectors (expressed in radians), because a small segment of a circle "almost" looks like a triangle.

```{r,engine='tikz',fig.width=1.5}
\usetikzlibrary{arrows.meta}
\definecolor{primary}{RGB}{177,98,78}
\definecolor{secondary}{RGB}{91,132,177}
\newcommand{\ket}[1]{|#1\rangle}
\begin{tikzpicture}
  \draw [thick,-Latex,primary] (0,0) to node[below right]{$u$} (2,2);
  \draw [thick,-Latex,rotate=25,secondary](0,0) to node[left]{$v$} (2,2);
  \draw [thick,-Latex] (2,2) to node[above right]{$v-u$} (.97,2.65);
\end{tikzpicture}
```

Alternatively (and more formally), we can see this by writing $|\braket{u}{v}|=\cos\alpha\approx1-\alpha^2/2$, since then
$$
  \|u-v\|
  = \sqrt{2(1-|\braket{u}{v}|)}
  \approx \alpha.
$$

This can certainly help with intuition, but extra care must always be taken when dealing with complex vector spaces, since our geometric intuition breaks down rapidly in (complex) dimension higher than $1$.
:::

As you might hope, two state vectors which are close to one another give similar statistical predictions.
In order to see this, pick a measurement (any measurement) and consider one partial outcome described by a projector $\proj{a}$.
What can we say about the difference between the two probabilities
$$
  \begin{aligned}
    p_u &= |\braket{a}{u}|^2
  \\p_v &= |\braket{a}{v}|^2
  \end{aligned}
$$
if we know that $\|u-v\|\leq\varepsilon$?

Well, first of all, let us introduce two classic tricks that are almost always useful when dealing with inequalities --- the first holds in any normed vector space, and the latter in any inner product space.

- the **reverse triangle inequality**:
    $$
      \Big|\|u\|-\|v\|\Big| \leq \|u-v\|
    $$
- the [**Cauchy--Schwartz inequality**](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality):^[This is arguably *the most useful* mathematical inequality that we have!]
    $$
      \braket{u}{v}^2 \leq \braket{u}{u}\braket{v}{v}
    $$
    or, equivalently (by taking square roots),
    $$
      |\braket{u}{v}| \leq \|u\|\|v\|.
    $$
    Furthermore, the two sides of the inequality are equal *if and only if* $\ket{u}$ and $\ket{v}$ are linearly dependent.

Using these, we see that
$$
  \begin{aligned}
    |p_u-p_v|
    &= \Big| |\braket{a}{u}|^2 - |\braket{a}{v}|^2 \Big|
  \\&= \Big| \Big( |\braket{a}{u}| + |\braket{a}{v}| \Big) \Big( |\braket{a}{u}| - |\braket{a}{v}| \Big) \Big|
  \\&\leq 2\Big| |\braket{a}{u}| - |\braket{a}{v}| \Big|
  \\&\leq 2\Big| \braket{a}{u} - \braket{a}{v} \Big|
  \\&\leq 2\|a\|\|u-v\|
  \\&= 2\|u-v\|.
  \end{aligned}
$$
So if $\|u-v\|\leq\varepsilon$, then $|p_u-p_v|\leq2\varepsilon$.

Again, we can appeal to some geometric intuition if we pretend that $\ket{u}$ and $\ket{v}$ are Euclidean vectors instead of rays.
Write
$$
  \begin{aligned}
    |\braket{a}{u}| &= \cos(\alpha)
  \\|\braket{a}{v}| &= \cos(\alpha+\varepsilon)
  \end{aligned}
$$
where $\varepsilon$ is the (very small) angle between $\ket{u}$ and $\ket{v}$, whence $\|u-v\|=\varepsilon$.
Then
$$
  \begin{aligned}
    |\braket{a}{u}|^2 - |\braket{a}{v}|^2
    &= \cos^2(\alpha) - \cos^2(\alpha+\varepsilon)
  \\&\approx \varepsilon\sin(2\alpha)
  \\&\leq \varepsilon.
  \end{aligned}
$$
As an interesting exercise, you might try to explain why this approach gives a tighter bound ($\varepsilon$ instead of $2\varepsilon$).


## Fidelity

Sometimes, when quantifying closeness of states, the *inner product* is a more convenient tool than the distance/norm.
Analogous to how we define the distance between states $\ket{u}$ and $\ket{v}$ as $d(u,v)=\|u-v\|$, we define the **fidelity** between them as
$$
  F(u,v)\coloneqq |\braket{u}{v}|^2.
$$
This is *not* a metric, but it does have some similarly nice properties: for example, $F(u,v)=1$ when the two states are identical, and $F(u,v)=0$ when the two states are orthogonal (which means that they are "as different as possible").
Intuitively, we can understand fidelity as the probability that the state $\ket{u}$ (resp. $\ket{v}$) would pass a test for being in state $\ket{v}$ (resp. $\ket{u}$).
In other words, if we perform an orthogonal measurement on $\ket{u}$ that has two outcomes ($\texttt{true}$ if the state is $\ket{v}$; $\texttt{false}$ if the state is orthogonal to $\ket{v}$), then the fidelity $F(u,v)=|\braket{u}{v}|^2$ is exactly the probability that we measure the outcome $\texttt{true}$.

Recall our definition of state distance:
$$
  d(u,v) = \sqrt{2(1-|\braket{u}{v}|)}
$$
This gives us a relation between distance and fidelity: once we know one, we can easily calculate the other.
However, everything we have said so far applies only to *pure* states --- we will see how the mixed state case is slightly more complicated shortly.

One final remark: as another example of the many inconsistencies in the literature, some authors define $F(u,v)$ to be $|\braket{u}{v}|$ instead of $|\braket{u}{v}|^2$.
Whenever we say fidelity, we mean the latter: $|\braket{u}{v}|^2$.


## Approximating unitaries

So now we know a bit about how norms (or metrics, or inner products) can help us to understand distance between state vectors, can we say something similar about quantum evolutions?
Say we have unitary operators $U$ and $V$ acting on the same Hilbert space, where $U$ is some "target" unitary that we *want* to implement in a real-life circuit, and $V$ is an "approximate" unitary that we *can actually* implement in practice.
We say that **$V$ approximates $U$ with precision $\varepsilon$** if
$$
  \|U-V\| \leq \varepsilon
$$
where $\|\cdot\|$ is some norm on unitary matrices (of the same size), which we would want to satisfy the property that, if $\|U-V\|$ is "small", then $U$ should be hard to distinguish from $V$, no matter on which quantum state they act.
One natural choice of norm which will be suitable for our purposes is the following.

::: {.idea latex=""}
The **operator norm** $\|A\|$ of an operator $A\in\mathcal{B}(\mathcal{H})$ is the maximum length of the vector $A\ket{v}$ over all possible normalised vectors $\ket{v}\in\mathcal{H}$, i.e.
$$
  \|A\| \coloneqq \max\Big\{|A\ket{v}| \text{ s.t. }|\ket{v}|=1\Big\}.
$$
One can show that $\|A\|$ is equal to the largest singular value^[We discussed singular values abstractly in Section \@ref(the-schmidt-decomposition), but we can be more explicit here: the **singular values** of an operator $A$ are the square roots of the (necessarily non-negative) eigenvalues of the Hermitian operator $A^\dagger A$.] of $A$.

If $A$ is *normal*, then its singular values are exactly the absolute values of its eigenvalues, and so
$$
  A\text{ normal }\implies
  \|A\| = \max\Big\{|\lambda| \text{ s.t. }\det(A-\lambda\id)=0\Big\}.
$$
:::

The operator norm satisfies some very useful properties:^[Proving these properties, along with some others, is a good thing to practise --- see Exercise \@ref(operator-norm).]

- If $A$ is normal, then $\|A^\dagger\|=\|A\|$
- $\|A\otimes B\|=\|A\|\|B\|$
- If $U$ is unitary, then $\|U\|=1$
- If $P\neq0$ is an orthogonal projector, then $\|P\|=1$
- **Sub-multiplicativity:** $\|AB\|\leq\|A\|\|B\|$.

Now suppose that
**TO-DO: continue from here**











## Distinguishing non-orthogonal states, again {#distinguishing-non-orthogonal-states-again}

Let's briefly return to the problem considered in Section @\ref(distinguishing-non-orthogonal-states).
**TO-DO: include end of ยง1 in the draft**


























## *Remarks and exercises* {#remarks-and-exercises-11}

### Hamming distance {#hamming-distance}

Show that the Hamming distance (defined in Section \@ref(metrics)) is indeed a metric.


### Operator norm {#operator-norm}

Prove the following properties of the operator norm:

2. $\|A\otimes B\|=\|A\|\|B\|$ for any operators $A$ and $B$
1. If $A$ is normal, then $\|A^\dagger\|=\|A\|$
3. If $U$ is unitary, then $\|U\|=1$
4. If $P\neq0$ is an orthogonal projector, then $\|P\|=1$.

Using the singular value decomposition^[Recall Section \@ref(the-schmidt-decomposition)], or otherwise, prove that the operator norm has the following two properties for any operators $A$ and $B$:

5. **Unitary invariance:** $\|UAV\|=\|A\|$ for any unitaries $U$ and $V$
6. **Sub-multiplicativity:** $\|AB\|\leq\|A\|\|B\|$.

Recall that we say that $V$ approximates $U$ with precision $\varepsilon$ if $\|U-V\|\leq\varepsilon$.

7. Prove that, if $V$ approximates $U$ with precision $\varepsilon$, then $V^{-1}$ approximates $U^{-1}$ with the same precision $\varepsilon$.

Using the Cauchy--Schwartz inequality, or otherwise, prove the following, for any vector $\ket{\psi}$ and any operators $A$ and $B$:

8. $|\braket{\psi|A^\dagger B}{\psi}|\leq\|A\|\|B\|$.


### Distinguishability and the trace distance

**TO-DO: check that the notation here (e.g. for trace distance) is consistent**

Say we have a physical system which is been prepared in one of two states (say, $\rho_1$ and $\rho_2$), each with equal probability.
Then a *single* measurement can distinguish between the two preparations with probability *at most* $\frac{1}{2}[1+d_{\tr}(\rho_1,\rho_2)]$.

1. Suppose that $\rho_1$ and $\rho_2$ commute.^[The commutativity assumption makes this problem essentially a special case of a purely classical one: distinguishing between two probability distributions.]
    Using the spectral decompositions of $\rho_1$ and $\rho_2$ in their common eigenbasis, describe the optimal measurement that can distinguish between the two states. What is its probability of success?

2. Suppose that you are given one randomly selected qubit from a pair in the state
    $$
      \ket{\psi} =
      \frac{1}{\sqrt{2}}\left(
        \ket{0}\otimes\left(
          \sqrt{\frac23}\ket{0}
          - \sqrt{\frac13}\ket{1}
        \right)
        + \ket{1}\otimes\left(
          \sqrt{\frac23}\ket{0}
          + \sqrt{\frac13}\ket{1}
        \right)
      \right)
    $$
    (which we have already seen in Exercise \@ref(some-density-operator-calculations)).
    What is the maximal probability with which you can determine which qubit (either the first or the second) you were given?
