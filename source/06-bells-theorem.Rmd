# (PART) Further foundations {-}

# Bell's theorem {#bells-theorem}

> About **quantum correlations**, which are stronger than any correlations allowed by classical physics, and about the **CHSH inequality** (used to prove a variant of **Bell's theorem**) which demonstrates this fact.

Every now and then, it is nice to put down your lecture notes and go and see how things actually work in the real world.
What is particularly wonderful (and maybe surprising) about quantum theory is that it turns up in many places where we might not expect it to.
One such example is in the polarisation of light, where we stumble across an intriguing paradox.

The (much-simplified) one sentence introduction to light polarisation is this: light is made of [**transverse**](https://en.wikipedia.org/wiki/Transverse_wave) waves, and transverse waves have a "direction", which we call **polarisation**; a **polarising filter** only allows waves of a certain polarisation to pass through.
If we take two polarising filters, and place them on top of each other with their polarisations oriented at $90^\circ$ to one another, then basically no light will pass through, since the only light that can pass through the first filter is orthogonally polarised with respect to the second filter, and is thus blocked from passing through.
But then, if we take a third filter, and place it in between the other two, at an angle in the middle of both (i.e. at $45^\circ$), then somehow *more* light is let through than if the middle filter weren't there at all.^[For the more visually inclined, there is a [video on YouTube by minutephysics](https://www.youtube.com/watch?v=zcqZHYo7ONs) about this experiment, or you can play with a virtual version on the [Quantum Flytrap Virtual Lab](https://lab.quantumflytrap.com/lab/bell-inequality?mode=laser).]

This is intrinsically linked to **Bell's theorem**, which proves the technical sounding statement that "any local real hidden variable theory must satisfy certain statistical properties", which is _not_ satisfied in reality, as many quantum mechanical experiments (such as the above) show!





## Hidden variables {#hidden-variables}

The story of "hidden variables" dates back to 1935 and grew out of Einstein's worries about the completeness of quantum theory.
Consider, for example, a single qubit.
Recalling our previous discussion on compatible operators (Section \@ref(compatible-observables-and-uncertainty)), we know that no quantum state of a qubit can be a simultaneous eigenstate of two *non-commuting* operators, such as $\sigma_x$ and $\sigma_z$.
Physically, this means that if the qubit has a definite value of $\sigma_x$ then its value of $\sigma_z$ must be indeterminate, and vice versa.
If we take quantum theory to be a complete description of the world, then we must accept that it is impossible for both $\sigma_x$ and $\sigma_z$ to have definite values for the same qubit at the same time.^[Here it's important that we're really talking about so-called **local** hidden variable theories. We discuss the technical details in \@ref(hidden-loopholes).]
Einstein felt very uncomfortable about all this: he argued that quantum theory is incomplete, and that observables $\sigma_x$ and $\sigma_z$ may both have simultaneous definite values, although we only have knowledge of one of them at a time.
This is the hypothesis of **hidden variables**.

In this view, the indeterminacy found in quantum theory is merely due to our ignorance of these "hidden variables" that are present in nature but not accounted for in the theory.
Einstein came up with a number of pretty good arguments for the existence of "hidden variables", perhaps the most compelling of which was described in his 1935 paper (known as "the EPR paper"), co-authored with his younger colleagues, Boris Podolsky and Nathan Rosen.
It stood for almost three decades as the most significant challenge to the completeness of quantum theory.
Then, in 1964, John Bell showed that the local^[This key word "local" is very important for those who care about the subtle technical details, but we won't explain it here.] hidden variable hypothesis can be tested and *refuted*.

::: {.idea latex=""}
Any theory can make predictions, but just because the predictions turn out to be correct, this does not make the theory true --- there may be other, maybe equivalent, explanations.
The key to the scientific method is **falsifiability**: make one prediction incorrectly, and you have proven your theory is not true.
:::

::: {.technical title="Hidden-variable no-go theorems" latex="{Hidden-variable no-go theorems}"}
We already saw some no-go theorems in Section \@ref(no-cloning-and-no-go) that set limits on what we can do with quantum states.
In this chapter we're going to see one no-go theorem relating to the *foundations* of quantum theory, in particular concerning these *local* "hidden variables".
Again, there are many related no-go theorems, and again they fall beyond the scope of this book, but it's worth mentioning them by name at least.
They all state that a certain type of (**realistic**, in some technical sense of the word) hidden-variable theory is inconsistent with reality:

- [**Bell's theorem**](https://en.wikipedia.org/wiki/Bell%27s_theorem) (which we will see in Section \@ref(bells-theorem-via-chsh)) is for **local** hidden-variable theories.
- The [**Kochen--Specker theorem**](https://en.wikipedia.org/wiki/Kochen–Specker_theorem) is for **non-contextual** hidden-variable theories.
- The [**Pusey--Barret--Rudolph theorem**](https://en.wikipedia.org/wiki/Pusey–Barrett–Rudolph_theorem) (often simply called the **PBR theorem**) is for **preparation independent** hidden-variable theories.

All together, these three theorems say that, if some hidden-variable theory does exist, then it has to be *non-local*, *contextual*, and *preparation dependent*.
But what do these words mean?

Preparation independence is the assumption that, if we independently prepare two quantum states, then their hidden variables are also independent.
Locality is the idea that things can only be directly affected by their surroundings, i.e. the exact opposite of "spooky action at a distance".
Contextuality is a bit more subtle, and can actually be seen as a direct generalisation of non-locality (by **Fine's theorem**), but it talks about how results of measurements depend on the commutator of the observable being measured, i.e. on its "context".

A particularly useful way of formally defining non-locality and contextuality is by using the language of [**sheaf theory**](https://en.wikipedia.org/wiki/Sheaf_(mathematics)), which is an inherently topological and category-theoretic notion.
This approach was cemented by Abramsky and Bradenburger's "The Sheaf-Theoretic Structure Of Non-Locality and Contextuality", arXiv:[1102.0264](https://arxiv.org/abs/1102.0264).
:::





## Quantum correlations {#quantum-correlations}

Consider two entangled qubits in the **singlet**^[We say that a system is **singlet** if all the qubits involved are entangled. For example, the Bell states (Section \@ref(bell-states)) are all (maximally entangled) singlet states. This is related to the notion of [**singlet states**](https://en.wikipedia.org/wiki/Singlet_state) in quantum mechanics, which are those with zero net angular momentum.] state
$$
  \ket{\psi}
  = \frac{1}{\sqrt{2}} \left( \ket{01}-\ket{10} \right)
$$
and note that the projector $\proj{\psi}$ can be written as
$$
  \proj{\psi}
  = \frac{1}{4} \left(
    \id\otimes\id - \sigma_x\otimes\sigma_x - \sigma_y\otimes\sigma_y - \sigma_z\otimes \sigma_z
  \right)
$$
where $\sigma_x$, $\sigma_y$, and $\sigma_z$ are our old friends the Pauli matrices.

Also recall that any single-qubit observable^[We say "observable" and "value" instead of "Hermitian operator" and "eigenvalue" because it's useful to be able to switch between speaking like a mathematician and like a physicist!] with values $\pm1$ can be represented by the operator
$$
  \vec{a}\cdot\vec\sigma
  = a_x\sigma_x + a_y\sigma_y + a_z\sigma_z,
$$
where $\vec{a}$ is a unit vector in the three-dimensional Euclidean space.

So if Alice and Bob both choose observables, then we can characterise their choice^[For example, if the two qubits are spin-half particles, they may measure the spin components along the directions $\vec{a}$ and $\vec{b}$.] by vectors $\vec{a}$ and $\vec{b}$, respectively.
If Alice measures the first qubit in our singlet state $\ket{\psi}$, and Bob the second, then the corresponding observable is described by the tensor product
$$
  A\otimes B
  = (\vec{a}\cdot\vec\sigma)\otimes(\vec{b}\cdot\vec\sigma).
$$
The eigenvalues of $A\otimes B$ are the products of eigenvalues of $A$ and $B$.
Thus $A\otimes B$ has two eigenvalues: $+1$, corresponding to the instances when Alice and Bob registered identical outcomes, i.e. $(+1,+1)$ or $(-1,-1)$; and $-1$, corresponding to the instances when Alice and Bob registered different outcomes, i.e. $(+1,-1)$ or $(-1,+1)$.

This means that the expected value^[Recall Section \@ref(observables): the expected value of an operator $E$ in the state $\ket{\phi}$ is equal to $\braket{\phi|E}{\phi}$.] of $A\otimes B$, in any state, has a simple interpretation:
$$
    \av{A\otimes B} = \Pr (\text{outcomes are the same}) - \Pr (\text{outcomes are different}).
$$
This expression can take any real value in the interval $[-1,1]$, where $-1$ means we have **perfect anti-correlations**, $0$ means **no correlations**, and $+1$ means **perfect correlations**.

We can evaluate the expectation value in the singlet state:
$$
\begin{aligned}
  \bra{\psi}A\otimes B\ket{\psi}
  & = \tr\Big[
      (\vec{a}\cdot\vec\sigma)\otimes(\vec{b}\cdot\vec\sigma) \proj{\psi}
    \Big]
\\& = -\frac{1}{4} \tr\Big[
      (\vec{a}\cdot\vec\sigma)\sigma_x \otimes(\vec{b}\cdot\vec\sigma)\sigma_x
      + (\vec{a}\cdot\vec\sigma)\sigma_y \otimes(\vec{b}\cdot\vec\sigma)\sigma_y
      + (\vec{a}\cdot\vec\sigma)\sigma_z \otimes(\vec{b}\cdot\vec\sigma)\sigma_z
    \Big]
\\& = -\frac{1}{4} \tr\Big[
      4(a_x b_x + a_y b_y + a_z b_z) \id\otimes\id
    \Big]
\\& = -\vec{a}\cdot\vec{b}
\end{aligned}
$$
where we have used the fact that $\tr(\vec{a}\cdot\vec\sigma)\sigma_k = 2a_k$ (for $k=x,y,z$).
So if Alice and Bob choose the same observable $\vec{a} = \vec{b}$, then the expected value $\av{A\otimes B}$ will be equal to $-1$, and their outcomes will *always* be opposite: whenever Alice registers $+1$ (resp. $-1$) Bob is bound to register $-1$ (resp. $+1$).





## The CHSH inequality {#chsh-inequality}

> An upper bound on **classical** correlations.

<div class="video" title="Device-independent tests and Bell inequalities" data-videoid="xFj9Mf9LGso"></div>

We will describe the most popular version of Bell's argument, introduced in 1969 by [John Clauser](https://en.wikipedia.org/wiki/John_Clauser), [Michael Horne](https://en.wikipedia.org/wiki/Michael_Horne_(physicist)), [Abner Shimony](https://en.wikipedia.org/wiki/Abner_Shimony), and [Richard Holt](https://en.wikipedia.org/wiki/Richard_Holt_(physicist)) (whence the name "CHSH").

Let us start by making this assumption that the results of any measurement on any individual system are predetermined --- any probabilities we may use to describe the system merely reflect our ignorance of these hidden variables.

Imagine the following scenario.
Alice and Bob, our two characters with a predilection for wacky experiments, are equipped with appropriate measuring devices and sent to two distant locations.
Assume that Alice and Bob each have a choice of *two* observables that they can measure, each with well defined^[The phrase "well defined" corresponds to our "hidden variable" assumption, i.e. that the observables *always* have *definite* values.] values $+1$ and $-1$.
Let's say that Alice can choose between observables $A_1$ and $A_2$, and Bob between $B_1$ and $B_2$.
Now, somewhere in between them there is a source that emits pairs of qubits that fly apart, one towards Alice and one towards Bob.
For each incoming qubit, Alice and Bob choose randomly, and independently from each other, which particular observable will be measured.
This means we can think of the observables as random variables $A_k,B_k$ (for $k=1,2$) that take values $\pm1$.
Using these, we can define a new random variable: the **CHSH quantity**
$$
  S = A_1(B_1 - B_2) + A_2(B_1 + B_2).
$$

By a case-by-case analysis of the four possible outcomes for the pair $(B_1,B_2)$, we see that one of the terms $B_1\pm B_2$ must be equal to zero and the other to $\pm 2$ (basically depending on if $B_1=B_2$ or not), and so (looking at the four possible outcomes for the pair $(A_1,A_2)$) we see that $S=\pm2$.
But the average value of $S$ must lie in between these two possible outcomes, i.e.
$$
  -2 \leq \av{S} \leq 2.
$$
That's it!
Such a simple and yet profound mathematical statement about correlations, which we refer simply to as the **CHSH inequality**.

::: {.idea latex=""}
There is absolutely *no quantum theory involved* in the CHSH inequality
$$
  -2 \leq \av{S} \leq 2
$$
because the CHSH inequality is not specific to quantum theory: it does not really matter what kind of physical process is behind the appearance of binary values of $A_1$, $A_2$, $B_1$, and $B_2$; it is merely a statement about correlations, and for all classical correlations we must have
$$
  |
    \av{A_1 B_1} - \av{A_1 B_2} + \av{A_2 B_1} + \av{A_2 B_2}
  | \leq 2
$$
(which is just another way of phrasing the CHSH inequality).
:::

There are essentially *two* (very important) assumptions here:

1. **Hidden variables.** Observables have definite values.
2. **Locality.** Alice's choice of measurements (choosing between $A_1$ and $A_2$) does not affect the outcomes of Bob's measurement, and vice versa.

We will not discuss the locality assumption right now in detail (see Section \@ref(hidden-loopholes)), but let us just give one brief comment.
In the hidden variable world a, statement such as "if Bob were to measure $B_1$ then he would register $+1$" must be either true or false (and not "undecidable" or some other such thing!) *prior to Bob's measurement*.
Without the locality hypothesis, such a statement is ambiguous, since the value of $B_1$ could depend on whether $A_1$ or $A_2$ will be chosen by Alice.
We do not want this since it implies *instantaneous communication* --- it means that, say, Alice by making a choice between $A_1$ and $A_2$ affects Bob's results: Bob can immediately "see" what Alice "does".

Now let's see how quantum theory *fundamentally disagrees* with the CHSH inequality.





## Bell's theorem via CHSH {#bells-theorem-via-chsh}

<div class="video" title="Device-independent tests and Bell inequalities (continued)" data-videoid="P-hk1GnAE6k"></div>

Continuing this story of Alice and Bob with their observables and pairs of qubits, let us first rephrase things in the formalism of quantum mechanics that we've been building up.
The observables $A_1$, $A_2$, $B_1$, $B_2$ become $(2\times 2)$ Hermitian matrices, each with the two eigenvalues $\pm 1$, and $\av{S}$ becomes the expected value of the $(4\times 4)$ **CHSH matrix**
$$
  S = A_1\otimes(B_1-B_2) + A_2\otimes(B_1+B_2).
$$
We can now evaluate $\av{S}$ using quantum theory.

::: {.idea latex=""}
Actually performing these measurements described by $S$ on a pair of qubits is known as a **CHSH test**, or **Bell test**.
:::

If the two qubits are in the singlet state
$$
  \ket{\psi}
  = \frac{1}{\sqrt{2}} \left( \ket{01}-\ket{10} \right)
$$
then we have already seen (Section \@ref(quantum-correlations)) that
$$
  \av{A\otimes B} = -\vec{a}\cdot\vec{b}.
$$
So if we choose vectors $\vec{a}_1$, $\vec{a}_2$, $\vec{b}_1$, and $\vec{b}_2$ as shown in Figure \@ref(fig:choice-of-as-and-bs), then the corresponding matrices^[That is, $A_1=\vec{a}_1\cdot\vec{\sigma}$, and so on.] satisfy
$$
\begin{aligned}
  \av{A_1\otimes B_1}
  &= \av{A_2\otimes B_1}
  = \av{A_2\otimes B_2}
  = \frac{1}{\sqrt{2}}
\\\av{A_1\otimes B_2}
  &= -\frac{1}{\sqrt{2}}.
\end{aligned}
$$

(ref:choice-of-as-and-bs-caption) The relative angle between the two perpendicular pairs is $45^\circ$.

```{r choice-of-as-and-bs,engine='tikz',fig.width=1.5,fig.cap='(ref:choice-of-as-and-bs-caption)'}
\definecolor{primary}{RGB}{177,98,78}
\definecolor{secondary}{RGB}{91,132,177}
\begin{tikzpicture}
  \draw [->,primary,rotate=45] (0,0) -- (1,0) node [right] {$b_1$};
  \draw [->,primary,rotate=-45] (0,0) -- (1,0) node [below] {$b_2$};
  \draw [->,secondary,rotate=90] (0,0) -- (1,0) node [above] {$a_1$};
  \draw [->,secondary,rotate=00] (0,0) -- (1,0) node [right] {$a_2$};
\end{tikzpicture}
```

Plugging these values in, we get that
$$
  \av{A_1 B_1} - \av{A_1 B_2} + \av{A_2 B_1} + \av{A_2 B_2}
  = -2\sqrt{2},
$$
which obviously violates CHSH inequality: $-2\sqrt{2}$ is strictly less than $-2$!

But here is the really important part of this discussion: *this violation of the CHSH has been observed in a number of painstakingly careful experiments --- this is not just theoretical!*
The early efforts in these experiments were truly heroic, with many many layers of complexity; today, however, such experiments are routine.

::: {.idea latex=""}
**Bell's theorem.**
The behaviour of entangled quantum systems *cannot* be explained by local hidden variables.
In other words, outcomes in quantum mechanics really are random, and it's not simply our lack of knowledge about some background process.
:::

If we can enforce locality in an experimental setup (for example, by ensuring that Alice and Bob are sufficiently far apart so that there is not enough time between Alice making a measurement and Bob receiving his measurement result) then an experimental verification of the CHSH test *proves* to us that the system is behaving in an inherently non-classical and, importantly, *unpredictable* manner.
This means that this is a good test to see if our devices are performing as they are supposed to, and are untampered by any potential eavesdroppers.^[If an eavesdropper has observed our system to the extent that they can predict out outcomes, then that very predictability means that there is a hidden-variable description of the system, and the CHSH inequality is not violated.]
In other words, the CHSH test is key for securing quantum protocols, as we will explain in Section \@ref(quantum-randomness).





## Tsirelson's inequality {#tsirelsons-inequality}

> An upper bound on **quantum** correlations.

One may ask if $|\av{S}|= 2\sqrt{2}$ is the *maximal* violation of the CHSH inequality, and the answer is "yes, it is": *quantum* correlations *always* satisfy the bound $|\av{S}|\leq2\sqrt{2}$.
This is because, no matter which state $\ket{\psi}$ we pick, the expected value $\av{S} = \braket{\psi}{S|\psi}$ cannot exceed the largest eigenvalue of $S$, and we can put an upper bound on the largest eigenvalues of $S$.
To start with, taking the largest eigenvalue (in absolute value) of a Hermitian matrix $M$, which we denote by $\|M\|$, gives a matrix norm, i.e. it has the following properties:
$$
\begin{aligned}
  \|M\otimes N\|
  & = \|M\| \|N\|
\\\|MN\|
  & \leq \|M\| \|N\|
\\\|M+N\|
  & \leq \|M\| + \|N\|
\end{aligned}
$$

Given that $\|A_k\|=\|B_k\|=1$ (for $k=1,2$), it is easy to use these properties to show that $\|S\|\leq4$, but this is a much weaker bound than we want.
However, one can show^[Exercise \@ref(proving-tsirelsons-inequality).] that
$$
  S^2
  = 4(\id\otimes\id) + [A_1,A_2]\otimes[B_1,B_2].
$$

Now, the norms of the commutators $\|[A_1, A_2]\|$ and $\|[B_1, B_2]\|$ are bounded by^[Exercise \@ref(proving-tsirelsons-inequality).] $2$, and $\|S^2\|=\|S\|^2$.
All together, this gives
$$
  \begin{aligned}
    \|S^2\|
    &\leq8
  \\\implies \|S\|
    &\leq2\sqrt{2}
  \\\implies |\av{S}|
    &\leq2\sqrt{2}
  \end{aligned}
$$
This result is known as the **Tsirelson inequality**.

::: {.idea latex=""}
In *classical* probability theory, the (absolute value of the) average value of the CHSH quantity
$$
  S = A_1(B_1 - B_2) + A_2(B_1 + B_2)
$$
is bounded by $2$, and this bound can be attained.

In *quantum* theory, the same value is bounded by $2\sqrt{2}$, and this bound can also be attained.
:::





## Quantum randomness {#quantum-randomness}

The experimental violations of the CHSH inequality have shown us that there are situations in which the measurement outcomes are truly unknown the instant before the measurement is made, and so the answer must be "chosen" randomly.
We can make use of this randomness in a number of different ways, the most obvious example of which being a random number generator.
Indeed, we have already met one suitable implementation:

```{r,engine='tikz',engine.opts=list(template="latex/tikz2pdf.tex"),fig.width=1.5}
\begin{quantikz}
  \lstick{$\ket{0}$} \qw
  & \gate{H}
  & \qw
\end{quantikz}
```

The state before measurement is $(\ket{0}+\ket{1})/\sqrt{2}$, so the two possible outcomes occur with equal probability.
This is a truly random number generator, not like the pseudorandom one that is used if you ask your computer for some random data.

This randomness generator works well as long as we know how it's been built, i.e. that it really is just a Hadamard gate, that the input qubit really has been prepared in the state $\ket{0}$, and that the measurement device is accurate and honest.
However, we don't all have a Hadamard gate and a supply of prepared qubits lying around at home, so it seems likely that at some point we might have to buy or borrow such a device from a third party.
But then how can we *know* that it really is doing what it promises, and not just supplying some pseudorandom numbers that might, for example, already be known to the manufacturer?
This would render the device useless for cryptographic purposes!
But not only do we want to know that this isn't the case, we would also like the average user to be able to verify this for themselves, without having to know about the internal details.
In other words, can we find a way of verifying the device via some analysis of just inputs and outputs?
This is the question of device independence.

::: {.idea latex=""}
A protocol is **device independent** if its security doesn't depend on trusting the devices on which it is implemented.
In other words, it has no reliance on trusting the third party who supply you with the devices.
:::

We can rule out one thing from the start, namely deterministic behaviour.
If we behave deterministically then we have no hope, since the third party can take this into account and potentially find a way to always fool us.
But there is another approach that we can try: rather than directly trying to verify the veracity of any given device, we can try to use it to turn a small amount of true randomness into a larger amount.
This is the idea of randomness expansion.

::: {.idea latex=""}
Starting from an initial seed of private randomness (completely unknown to any other party), **randomness expansion** is the process of extending this to a larger amount of randomness that remains completely private.
:::

Let's consider a different device: one that produces pairs of qubits in singlet states and gives one of its qubits to Alice and one to Bob.
If Alice then measures her qubit in the $X$ basis, and Bob measures his in the $Z$ basis (each keeping their outcomes private), they each obtain random bits that are independent of one another.
However, this is only really true if the device truly is giving them singlet states, and not predetermined unentangled states.
How can they test for this?

Using the idea of randomness expansion, let's assume that they start with some shared random private seed: some $m$-bit string that only they know.^[Note that we're pushing the problem somewhere else: how can they come up with this shared private seed in the first place? This is the problem of **key distribution**, and we'll return to this again later.]
They start by generating $n$ of these putative singlet states, and publicly decide on some value $0<p<1$.
With this, they randomly select $\lceil pn\rceil$ of the pairs to perform a CHSH test on.
Each test requires two random bits (to determine Alice and Bob's choice of measurement), so in total we will need the length $m$ of their shared random private seed be roughly
$$
  m
  \approx 2pn - pn\log_2 p - n(1-p)\log_2(1-p)
$$
where the $\log$ terms are approximately how many bits are required to randomly choose the subset of pairs to test.

Why does this help?
Well, if somebody has manipulated the device that produces the pairs, then they need to be sure that they haven't altered the pairs that Alice and Bob are testing on.
But they cannot know in advance which pairs that will be, and so they cannot risk manipulating anything.^[One can be much more quantitative about this by using Chernoff bounds for a simple strategy of "choose at random which pairs to manipulate", but a full proof of security is much more involved than we would like to be here.]

In the fraction $p$ of pairs where a CHSH test is performed, we get at least 1 bit of randomness out: Alice's measurement result is always chosen at random.
In fact, Bob's result will be partially correlated to Alice's, so we should be able to extract some more randomness from this as well, but we ignore this possibility for the sake of simplicity.
Thus in the end we create $(2-p)n$ bits of randomness.





## Loopholes in Bell tests {#hidden-loopholes}

When we introduced the idea of hidden variable theories in Section \@ref(hidden-variables), we made some assumptions to simplify the exposition, but these have a big impact on the practical reality of violating Bell tests.
Any test that does not satisfy one or more of these assumptions is said to have a **loophole**.
For verifying fundamental physics, we are not so worried about these loopholes --- it feels very unlikely that the putative classicality of the experiment is hiding in whatever loophole might be available in a given system.
But for cryptographic purposes, an adversary will use and loophole at their disposal to try to trick you!

There are three types of key assumptions that we will talk about here, and for each one we provide some exercises to work through in order to explore them further:

- detector efficiency (Exercise \@ref(detector-loophole))
- locality (Exercise \@ref(locality-loophole))
- free will (Exercise \@ref(free-will-loophole)).

Let's start with the first, which gives rise to the **detector loophole**.
When we make a measurement with a real-life device, in practice it doesn't always work --- maybe it just fails to notice a photon flying past.
Each detector has a parameter $\eta$ known as its **efficiency**: $\eta$ is the probability that the measurement succeeds.
For testing fundamental physics, it seems reasonable to assume that the successful measurements are a fair sample of what's really going on.
But if there's an adversary, they might substitute our detectors for completely perfect one, and then deliberately choose to fake a failure whenever their eavesdropping attempts fail.

The next crucial assumption in the CHSH test is that Alice and Bob are separated by a "large enough" distance in space and time.
More precisely, if they are physical at distance $L$ from each other, then their random choices of measurement setting, followed by their corresponding carrying out of the measurement, and receipt of the answers, should all be accomplished within a time approximately^[We say "approximately" here because we are avoiding being specific about how we actually define distance.] $L/c$ of each other, where $c$ is the speed of light.
If Alice and Bob are not far enough away from each other, then they are said to be within each other's **locality**, and so this is known as the **locality loophole**.

The final important assumption that we will mention here involves the availability of true randomness, and emphasises the importance of randomness expansion.
It asserts that Alice and Bob must be able to choose their measurement settings randomly.
This freedom to make their own choices is glibly referred to as them having "free will", and so this is known as the **free-will loophole**.
Resolving the locality loophole puts extremely tight constraints on how quickly choices must be made,^[If we say that Alice and Bob are $L=30\,\mathrm{km}$ apart from each other, then we're talking of timescales on the order of $10^{-4}\,\mathrm{s}$, which is not very long!] to the extent that Alice and Bob cannot make those choices manually --- they need to use random number generators.
But then they need to able to trust that these generators are indeed random, not merely pseudorandom, otherwise somebody else could know the origin of the "random" numbers and use that information to their advantage.





## *Remarks and exercises* {#remarks-and-exercises-bell}


### XOR games {#xor-games}

The setup of the CHSH inequality that we have described can instead be imagined as a two-player all-or-nothing game between Alice and Bob, so let's study this type of game more generally.

- Alice and Bob each start with an integer **prompt** $a$ and $b$ (respectively), with $1\leq a,b\leq n$.
  This integer can come from anywhere: they could pick it themselves, or it could be given to them.
  Having seen this integer, a round of the game consists of them returning an **answer**, $x$ and $y$ (respectively), of length $m$ bits to an oracle.
  Both the prompt and the answer are kept secret, so that only the corresponding player knows them.

- They win if the **winning condition** computed by the oracle is $1$, and lose if it is $0$.
  The winning condition is given by
  $$
    \begin{cases}
      1 & \text{if }g_A(x,a,b) = g_B(y,a,b)
    \\0 & \text{if }g_A(x,a,b) \neq g_B(y,a,b)
    \end{cases}
  $$
  where $g_A$ and $g_B$ are deterministic one-bit-valued functions whose output values are equally likely to be $0$ or $1$ (i.e. they return the value $0$ for exactly half of the possible input triples).

- There exists a *quantum* strategy that always wins.
  It uses sets of $m$ measurements on a maximally entangled state, and the measurement operators for all possible settings either commute or anticommute with one another.
  The measurements are specified by observables that are traceless (i.e. their trace is equal to $0$) and square to the identity.

- The best possible classical strategy fails $f$ of the time, for some fraction $f$.


### XOR games for quantum key distribution {#xor-games-for-qkd}

We can use the setup from Exercise \@ref(xor-games) as the basis for a **quantum key distribution scheme**: a process that allows two people to produce a shared random string known only to them.

For the $i$-th round of the game, Alice and Bob randomly (and privately) select their prompts $a_i$ and $b_i$, which they then use to play one round of the game, giving answers $x_i$ and $y_i$.
They keep a note of all their prompts and answers, as well as the result of the winning condition for each round.
After many rounds, Alice and Bob publicly announce all their prompts $\{a_1,\ldots,a_n\}$ and $\{b_1,\ldots,b_n\}$.

This means that Alice, for example, now knows the following:^[Bob knows the same, but instead of Alice's answers $\{x_1,\ldots,x_n\}$ he knows his own $\{y_1,\ldots,y_n\}$.]

- both sets of prompts $\{a_1,\ldots,a_n\}$ and $\{b_1,\ldots,b_n\}$
- her answers $\{x_1,\ldots,x_n\}$
- the values of $i\in\{1,\ldots,n\}$ for which $g_A(x_i,a_i,b_i)=g_B(y_i,a_i,b_i)$, i.e. the numbers $i_1,\ldots,i_k$ of the rounds that they won.

This is enough information (given that we have run enough rounds) for Alice to determine $g_A$ and Bob to determine $g_B$.
The two of them can then each compute the $k$-bit string
$$
  g_A(x_{i_1},a_{i_1},b_{i_1})\ldots g_A(x_{i_k},a_{i_k},b_{i_k})
  = g_B(x_{i_1},a_{i_1},b_{i_1})\ldots g_B(x_{i_k},a_{i_k},b_{i_k})
$$
which gives their shared key.
Now they just need to deal with eavesdropping.

By publicly selecting a random selection of rounds and announcing the results of their putative $g_A$ and $g_B$ for these rounds, they can check whether or not their values are coherent with the result of the winning conditions determined by the oracle.^[For example, if they know that they lost round $i$, then it should be the case that $g_A(x_i,a_i,b_i)\neq g_B(y_i,a_i,b_i)$.]

- If everything is coherent with the results, then they can be sure (if they have played enough rounds and compared enough results) that there was no eavesdropping.
- If the fraction of tests that are coherent with the results is less than $f$, then they must assume that somebody has been eavesdropping, and they should cease communication.
- Anywhere in between these two cases, they can quantify how much an eavesdropper might know, and then run some method of privacy amplification to further exclude the eavesdropper (at the cost of shortening the key).


### XOR games for randomness expansion

Consider again the scenario of Exercise \@ref(xor-games).
Explain how Alice by herself can treat this as a single-player game and use it as the basis for a randomness expansion scheme.^[*Hint: unlike in Exercise \@ref(xor-games-for-qkd), Alice no longer needs to choose a random subset of rounds to check the winning condition for: she can check all of them.*]


### Prescribed binary randomness

Find a quantum circuit^[*Hint: what is $\cos^2(\pi/4)$*?] that can act as a random source that outputs $0$ with probability $(2+\sqrt{2})/4$ and $1$ with probability $(2-\sqrt{2})/4$.


### Unbiasing bias

Suppose you have a source that produces random bits, but operates with a bias: it outputs $0$ with probability $p$ and $1$ with probability $1-p$, for some fixed $0<p<1$.

Find a method such that, given two outputs from this source, you successfully obtain a single unbiased random bit (i.e. as if the source had $p=1/2$) with probability $2p(1-p)$.


### Proving Tsirelson's inequality {#proving-tsirelsons-inequality}

Let $A_i$, $B_i$, and $\|-\|$ be as in Section \@ref(tsirelsons-inequality).

1. Prove that
  $$
    (A_1\otimes(B_1-B_2)+A_2\otimes(B_1+B_2))^2
    = 4(\id\otimes\id) + [A_1,A_2]\otimes[B_1,B_2].
  $$
2. Prove that
  $$
    \|[A_1,A_2]\|
    \leq 2
  $$
  (and the same argument should also apply to $\|[B_1,B_2]\|$).


### Detector loophole {#detector-loophole}

Say we have a detector with efficiency $\eta$, and an otherwise perfect CHSH test with $\av{S}=2\sqrt{2}$.

1. With what probability do both detections succeed? With what probability does exactly one detection fail? With what probability do both detectors fail?
2. Imagine that one detector successfully measures a qubit of a Bell state, while the other detector fails and notifies us of this fact.
  If we replace the reading of the failed detector by $+1$, what is the average value of the outcome?
3. Now imagine that both detections fail, and both readings are replaced by $+1$.
  What is the average value of the outcome?
4. Using the above, show that the critical detector efficiency for being able to rely on the outcome of the CHSH test is given by^[*Hint: if both detections succeed, then we can achieve $\av{S}=2\sqrt{2}$; the previous questions calculate $\av{S}$ in the other possible cases; so what is the sum of these values, weighted by their probabilities of occurring?*]
  $$
    \eta
    = 2(\sqrt{2}-1).
  $$


### Locality loophole {#locality-loophole}

1. Imagine that Alice and Bob are not very far from each other, and that they are going to perform a CHSH test using devices given to them by Eve, who has tampered with the devices and knows *both* of the measurement settings.
  How can Eve make Alice and Bob believe that they are sharing Bell pairs when, in reality, they are not?

To really get into the details of locality, we need to use some tools from [**special relativity**](https://en.wikipedia.org/wiki/Special_relativity): the theory of how non-accelerating observers measure times and distances.
The main assertion of special relativity is that *nothing can travel faster than the speed of light*.
These next exercises will be much easier if you have already seen things such as Minkowski diagrams before, but do not worry if you haven't.

2. We are going to plot a **Minkowski diagram** of the CHSH test scenario.
  This is a plot of physical position along the horizontal axis^[To make things easy, we assume that space is one-dimensional: Alice and Bob live on a line.] against time on the vertical axis.
  We place one **event** --- let's pick Alice choosing her measurement setting and getting a measurement result --- at the origin.
  Include on this diagram all the "places" (a pair consisting of a space coordinate and a time coordinate) that can receive a message about what measurement setting Alice chose, appealing to the main assertion of special relativity.
  This set of places is called the **future** of the event.

3. Add to the Minkowski diagram all the places that can send a message that could influence Alice's outcomes.
  This set of places is called the **past** of the event.

4. If an event (such as Bob choosing a measurement setting and getting a result) occurs in a region that is in neither the future nor the past of the event at the origin, what influence can these two events have over one another?
  If Bob's event is at a distance $L$ along the $x$-axis from the origin, then what is the maximal permissible time difference between the two events?^[*Hint: we already said in Section \@ref(hidden-loopholes) that the maximal permissible time difference should be $L/c$, so prove this.*]

5. If we wish to be really careful, then we should separate out the four events:
  
    a. Alice chooses a measurement setting
    b. Alice gets a measurement result
    c. Bob chooses a measurement setting
    d. Bob gets a measurement result.

    Draw a Minkowski diagram that includes all four events.
    Assuming that Alice and Bob are stationary, use this diagram to more explicitly describe the timing constraints of when each event should happen relative to one another if we wish to avoid the locality loophole.


### Free-will loophole {#free-will-loophole}

1. Assume that Eve, the manufacturer of the devices performing the CHSH test, knows what settings Alice and Bob are going to use.
  Explain how Eve can have faked the outputs, making the predetermined, while still seeming to be producing results consistent with quantum violation of the CHSH inequality.^[Depending on how you answered Exercise \@ref(locality-loophole), you might be able to use exactly the same idea here.]

2. Say that Eve only knows a fraction $p$ of the random outcomes (separately for both Alice and Bob).
  What is the maximum value of $p$ that still allows $\av{S}=2\sqrt{2}$?
  What about merely allowing $\av{S}>2$?
  (Assume that, whenever at least one random number is known, both devices know that value, and also know that they don't know the other value, but that one device will learn it when Alice or Bob chooses the measurement setting).

3. Imagine that Alice has a string of length $k$ of (apparently) randomly chosen bits.
  She believes that Eve knows a fraction $p$ of these bits.
  In an attempt to thwart Eve's attempts at eavesdropping, Alice computes the addition modulo 2 of all $k$ bits, resulting in a single bit.
  What is the probability that Eve can know this final value?
